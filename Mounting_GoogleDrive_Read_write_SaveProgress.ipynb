{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mounting_GoogleDrive_Read_write_SaveProgress.ipynb",
      "provenance": [],
      "mount_file_id": "1IumndJWbgTMVFltnWikNJeSGVTZHhLjD",
      "authorship_tag": "ABX9TyPffpvHGHi04K8zY3HrH5+0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/engbasem/ML-Taining-Notebooks/blob/main/Mounting_GoogleDrive_Read_write_SaveProgress.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mounting Google drive to Colab instance**\n",
        "\n",
        "[R1] https://machinelearningmastery.com/google-colab-for-machine-learning-projects/\n",
        "\n",
        "If you prefer storing all your files in Google drive, you can easily mount it onto the current instance of Colab. This will enable you to access all the datasets and files that are stored in your drive.\n",
        "\n",
        "There are a couple of ways you can do this:\n",
        "\n",
        "* You can click on the 'drive' icon in the 'Files' tab and follow the prompts on the screen."
      ],
      "metadata": {
        "id": "KBZGLEMa-HUw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  You can also mount your Google drive to the current Colab instance, by running the following lines of code in a code cell in your notebook."
      ],
      "metadata": {
        "id": "QOIaf8HP-bFy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_bxG6_e49nF",
        "outputId": "4ec75dfd-45ba-452f-9e60-50b81e33d590"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        " \n",
        "MOUNTPOINT = \"/content/gdrive\"\n",
        "DATADIR = os.path.join(MOUNTPOINT, \"MyDrive\")\n",
        "drive.mount(MOUNTPOINT)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, to write a file to our Google Drive, we can do the following. This code snippet writes Hello World! to a test.txt file in the top level of your Google Drive."
      ],
      "metadata": {
        "id": "P0YX1JQdZ7kJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# writes directly to google drive\n",
        "with open(f\"{DATADIR}/test.txt\", \"w\") as outfile:\n",
        "    outfile.write(\"Hello World!\")"
      ],
      "metadata": {
        "id": "Q02WkJ1OZ9Cg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, we can read from a file in our Google Drive as well by using:"
      ],
      "metadata": {
        "id": "tscepM40aKc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f\"{DATADIR}/test.txt\", \"r\") as infile:\n",
        "    file_data = infile.read()\n",
        "    print(file_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAJ3GtVdaNY8",
        "outputId": "6e9f66f2-1312-4fc6-8637-20b683331671"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello World!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Saving Model Progress on Google Drive**\n",
        "\n",
        "Google Colab is probably the easiest way to give us powerful GPU resources for your machine learning project. But in the free version of Colab, Google limits the time we can use our Colab notebook in each session. Our kernel may terminate for no reason. We can restart our notebook and continue our work, but we may lose everything in the memory. This is a problem if we need to train our model for a long time. Our Colab instance may terminate before the training is completed."
      ],
      "metadata": {
        "id": "p5IT8cw9akAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mounting Google Drive and using Keras ModelCheckpoint callback**\n",
        "\n",
        "we can save our model progress on Google Drive. This is particularly useful to work around Colab timeouts. It is more lenient for paid Pro and Pro+ users, but there is always a chance that our model training terminates midway at random times. \n",
        "\n",
        "**It is valuable if we donâ€™t want to lose our partially trained model.**"
      ],
      "metadata": {
        "id": "vJilhIAlauRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example using LENet5 on mnist dataset\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.layers import Input, Dense, Conv2D, Flatten, MaxPool2D\n",
        "from keras.models import Model\n",
        " \n",
        "class LeNet5(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(LeNet5, self).__init__()\n",
        "    #creating layers in initializer\n",
        "    self.conv1 = Conv2D(filters=6, kernel_size=(5,5), padding=\"same\", activation=\"relu\")\n",
        "    self.max_pool2x2 = MaxPool2D(pool_size=(2,2))\n",
        "    self.conv2 = Conv2D(filters=16, kernel_size=(5,5), padding=\"same\", activation=\"relu\")\n",
        "    self.flatten = Flatten()\n",
        "    self.fc1 = Dense(units=120, activation=\"relu\")\n",
        "    self.fc2 = Dense(units=84, activation=\"relu\")\n",
        "    self.fc3=Dense(units=10, activation=\"softmax\")\n",
        "  def call(self, input_tensor):\n",
        "    conv1 = self.conv1(input_tensor)\n",
        "    maxpool1 = self.max_pool2x2(conv1)\n",
        "    conv2 = self.conv2(maxpool1)\n",
        "    maxpool2 = self.max_pool2x2(conv2)\n",
        "    flatten = self.flatten(maxpool2)\n",
        "    fc1 = self.fc1(flatten)\n",
        "    fc2 = self.fc2(fc1)\n",
        "    fc3 = self.fc3(fc2)\n",
        "    return fc3"
      ],
      "metadata": {
        "id": "paTCDrrDbT_v"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you must mount GoogleDrive first\n",
        "import os\n",
        "from google.colab import drive\n",
        "MOUNTPOINT = \"/content/drive\"\n",
        "DATADIR = os.path.join(MOUNTPOINT, \"MyDrive\")\n",
        "drive.mount(MOUNTPOINT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oL6pZXRb5rC",
        "outputId": "28c7c959-95c6-4b25-a3bf-8d2ea6497633"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Afterward, we declare the Callback to save our checkpoint model to Google Drive.\n",
        "import tensorflow as tf\n",
        " \n",
        "checkpoint_path = DATADIR + \"/checkpoints/cp-epoch-{epoch}.ckpt\"\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)"
      ],
      "metadata": {
        "id": "AGv-_-gRcFFk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, we begin training on the MNIST dataset with the checkpoint callbacks\n",
        "# to ensure we can resume at the last epoch should our Colab session time out:\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.layers import Input, Dense, Conv2D, Flatten, MaxPool2D\n",
        "from keras.models import Model\n",
        " \n",
        "mnist_digits = keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist_digits.load_data()\n",
        " \n",
        "input_layer = Input(shape=(28,28,1))\n",
        "model = LeNet5()(input_layer)\n",
        "model = Model(inputs=input_layer, outputs=model)\n",
        "model.compile(optimizer=\"adam\", loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=\"acc\")\n",
        "model.fit(x=train_images, y=train_labels, batch_size=256, validation_data = [test_images, test_labels], epochs=5, callbacks=[cp_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MagiH_6_cTiM",
        "outputId": "5b3435bc-c843-485e-9222-fa0d03bd4476"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.9866 - acc: 0.8714\n",
            "Epoch 1: saving model to /content/drive/MyDrive/checkpoints/cp-epoch-1.ckpt\n",
            "235/235 [==============================] - 36s 151ms/step - loss: 0.9866 - acc: 0.8714 - val_loss: 0.1623 - val_acc: 0.9512\n",
            "Epoch 2/5\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.1320 - acc: 0.9615\n",
            "Epoch 2: saving model to /content/drive/MyDrive/checkpoints/cp-epoch-2.ckpt\n",
            "235/235 [==============================] - 35s 150ms/step - loss: 0.1320 - acc: 0.9615 - val_loss: 0.1032 - val_acc: 0.9686\n",
            "Epoch 3/5\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.0808 - acc: 0.9762\n",
            "Epoch 3: saving model to /content/drive/MyDrive/checkpoints/cp-epoch-3.ckpt\n",
            "235/235 [==============================] - 35s 151ms/step - loss: 0.0808 - acc: 0.9762 - val_loss: 0.0938 - val_acc: 0.9715\n",
            "Epoch 4/5\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.0575 - acc: 0.9822\n",
            "Epoch 4: saving model to /content/drive/MyDrive/checkpoints/cp-epoch-4.ckpt\n",
            "235/235 [==============================] - 35s 151ms/step - loss: 0.0575 - acc: 0.9822 - val_loss: 0.0688 - val_acc: 0.9798\n",
            "Epoch 5/5\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.0412 - acc: 0.9877\n",
            "Epoch 5: saving model to /content/drive/MyDrive/checkpoints/cp-epoch-5.ckpt\n",
            "235/235 [==============================] - 35s 151ms/step - loss: 0.0412 - acc: 0.9877 - val_loss: 0.0681 - val_acc: 0.9817\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc0d87dc390>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If model training stops midway, we can continue by just recompiling the model and loading the weights, and then we can continue our training:**\n"
      ],
      "metadata": {
        "id": "xVgvCkqFebRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = DATADIR + \"/checkpoints/cp-epoch-{epoch}.ckpt\"\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)\n",
        " \n",
        "input_layer = Input(shape=(28,28,1))\n",
        "model = LeNet5()(input_layer)\n",
        "model = Model(inputs=input_layer, outputs=model)\n",
        "model.compile(optimizer=\"adam\", loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=\"acc\")\n",
        " \n",
        "# to resume from epoch 5 checkpoints\n",
        "model.load_weights(DATADIR + \"/checkpoints/cp-epoch-5.ckpt\")\n",
        " \n",
        "# continue training\n",
        "model.fit(x=train_images, y=train_labels, batch_size=256, validation_data = [test_images, test_labels], \n",
        "          epochs=5, callbacks=[cp_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Voz86o-LcJNT",
        "outputId": "08dfc280-580c-41d5-b925-e5042bdb88fb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.0330 - acc: 0.9891\n",
            "Epoch 1: saving model to /content/drive/MyDrive/checkpoints/cp-epoch-1.ckpt\n",
            "235/235 [==============================] - 36s 151ms/step - loss: 0.0330 - acc: 0.9891 - val_loss: 0.0645 - val_acc: 0.9832\n",
            "Epoch 2/5\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.0273 - acc: 0.9914\n",
            "Epoch 2: saving model to /content/drive/MyDrive/checkpoints/cp-epoch-2.ckpt\n",
            "235/235 [==============================] - 36s 151ms/step - loss: 0.0273 - acc: 0.9914 - val_loss: 0.0594 - val_acc: 0.9833\n",
            "Epoch 3/5\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.0202 - acc: 0.9933\n",
            "Epoch 3: saving model to /content/drive/MyDrive/checkpoints/cp-epoch-3.ckpt\n",
            "235/235 [==============================] - 36s 152ms/step - loss: 0.0202 - acc: 0.9933 - val_loss: 0.0689 - val_acc: 0.9814\n",
            "Epoch 4/5\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.0234 - acc: 0.9916\n",
            "Epoch 4: saving model to /content/drive/MyDrive/checkpoints/cp-epoch-4.ckpt\n",
            "235/235 [==============================] - 36s 152ms/step - loss: 0.0234 - acc: 0.9916 - val_loss: 0.0695 - val_acc: 0.9834\n",
            "Epoch 5/5\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.0199 - acc: 0.9935\n",
            "Epoch 5: saving model to /content/drive/MyDrive/checkpoints/cp-epoch-5.ckpt\n",
            "235/235 [==============================] - 36s 152ms/step - loss: 0.0199 - acc: 0.9935 - val_loss: 0.0686 - val_acc: 0.9821\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc0d85c71d0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}